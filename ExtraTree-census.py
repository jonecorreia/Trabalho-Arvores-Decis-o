# -*- coding: utf-8 -*-
"""arvoreDecisaoCensus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PepPo0V1LGyVudGYyjBeWvhJZ-oNHQ5Z

# Arvore de Decisao
"""

!pip -q install plotly --upgrade
!pip -q install yellowbrick

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from google.colab import drive
drive.mount('/content/drive')

base_census = pd.read_csv('/content/census.csv')

#opc
base_census

#Divisão entre previsores e classe
#exibir nome das colunas, opc
base_census.columns

#separar os dados x para previsores e y para classes. iloc transforma em array numpy
X_census = base_census.iloc[:, 0:14].values
X_census

#y para classes. iloc cria array
y_census = base_census.iloc[:, 14].values
y_census

#Tratamento de atributos categóricos [dividido em 2 etapas: labelEncoder e OneHotEncoder]
#1 parte: LabelEncoder [ tranforma strings em indices numericos]
from sklearn.preprocessing import LabelEncoder

#exibe linha de registro do bd com diversos tipos de dados, numericos e strings.
X_census[0]

#criar um objeto p cada coluna q vai ser alterada [cjto de indices 1: masculino, 2: feminino]
label_encoder_workclass = LabelEncoder()
label_encoder_education = LabelEncoder()
label_encoder_marital = LabelEncoder()
label_encoder_occupation = LabelEncoder()
label_encoder_relationship = LabelEncoder()
label_encoder_race = LabelEncoder()
label_encoder_sex = LabelEncoder()
label_encoder_country = LabelEncoder()

#substitue nas colunas, conforme a posição no bd
#função fit.transform p mudar de string p mnumero
X_census[:,1] = label_encoder_workclass.fit_transform(X_census[:,1])
X_census[:,3] = label_encoder_education.fit_transform(X_census[:,3])
X_census[:,5] = label_encoder_marital.fit_transform(X_census[:,5])
X_census[:,6] = label_encoder_occupation.fit_transform(X_census[:,6])
X_census[:,7] = label_encoder_relationship.fit_transform(X_census[:,7])
X_census[:,8] = label_encoder_race.fit_transform(X_census[:,8])
X_census[:,9] = label_encoder_sex.fit_transform(X_census[:,9])
X_census[:,13] = label_encoder_country.fit_transform(X_census[:,13])
X_census[0]

# temmos um array só com numericos
X_census

#parte 2: 
#OneHotEncoder: cria novas colunas com base nos indices
##tecnica p calcular pesos das colunas por codificacao dos elementos, [array 1,2,3: o 3 tem mais peso q o 1]. 

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

onehotencoder_census = ColumnTransformer(transformers=[('OneHot', OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')

X_census = onehotencoder_census.fit_transform(X_census).toarray()

X_census

X_census[0]

X_census.shape

#Escalonamento dos valores
#deixar valores na mesma escala. p evitar o peso nas colunas[3+ importante 1]
#Standardization [padronização] ou normalização[normalization]
#Padronização é mais indicada qdo temos outliers[registros fora do padrão]

from sklearn.preprocessing import StandardScaler
scaler_census = StandardScaler()
X_census = scaler_census.fit_transform(X_census)

X_census[0]

#Divisão das bases em treinamento e teste
from sklearn.model_selection import train_test_split

#cria as variaveis p treinamento e teste. 
#x atributos previsores, y p classes. test-size 15% p testar,85% p treinar 
X_census_treinamento, X_census_teste, y_census_treinamento, y_census_teste = train_test_split(X_census, y_census, test_size = 0.15, random_state = 0)

#x atributos previsores, y p classes.
#exibe linhasxcol, #85% dados treinamento [sem oneHotEncode: 14 colunas, com OHE:116 col]
X_census_treinamento.shape, y_census_treinamento.shape

#15% dados p teste,lin x col, [sem OHE: 14, com OHE: 116]
X_census_teste.shape, y_census_teste.shape

#Salvar as variáveis
import pickle

#crio arquivo .pkl 
with open('census.pkl', mode = 'wb') as f:
  pickle.dump([X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste], f)

#--arvore de decisão--#
#carregar a bib sklearn
from sklearn.tree import DecisionTreeClassifier

import pickle
with open('census.pkl', 'rb') as f:
  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)

X_census_treinamento

y_census_treinamento

#fit define a arvore dec. atrib previsores, classe e realiza o treinamento
arvore_census = DecisionTreeClassifier(criterion='entropy',max_depth=3, random_state = 0)
arvore_census.fit(X_census_treinamento, y_census_treinamento)

#testes
#Naïve Bayes
from sklearn.naive_bayes import GaussianNB

with open('census.pkl', 'rb') as f:
  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)

X_census_treinamento.shape, y_census_treinamento.shape

X_census_teste.shape, y_census_teste.shape

#predição do alg
naive_census = GaussianNB()
naive_census.fit(X_census_treinamento, y_census_treinamento)
previsoes = naive_census.predict(X_census_teste)
previsoes

#compara com bd orginal
y_census_teste

#acuracia
#gerar a metricas p calcular a acuracia ou tx de acerto
from sklearn.metrics import accuracy_score, classification_report

#calcula acuracia
accuracy_score(y_census_teste, previsoes) # não executar o escalonamento

#gerar  a matriz de confusão do naive bayes
from yellowbrick.classifier import ConfusionMatrix

cm = ConfusionMatrix(naive_census)
cm.fit(X_census_treinamento, y_census_treinamento)
cm.score(X_census_teste, y_census_teste)

print(classification_report(y_census_teste, previsoes))

#=========#
#retorna a importancia de caada atributo depois dos calculos do maior ganho de inform
arvore_census.feature_importances_

#exibe as classes da arvore
arvore_census.classes_

#desenha a arvore
from sklearn import tree
figura, eixos = plt.subplots(nrows=1, ncols=1, figsize=(20,20))
tree.plot_tree(arvore_census, class_names=arvore_census.classes_, filled=True);



"""# Random Tree"""

##---------------##
##---------------##
##-arvore random tree##
##---------------##
##---------------##

#cria as variaveis p treinamento e teste. 
#x atributos previsores, y p classes. test-size 15% p testar,85% p treinar 
X_census_treinamento1, X_census_teste1, y_census_treinamento1, y_census_teste1 = train_test_split(X_census, y_census, test_size = 0.15, random_state = 0)

#x atributos previsores, y p classes.
#exibe linhasxcol, #85% dados treinamento [sem oneHotEncode: 14 colunas, com OHE:116 col]
X_census_treinamento1.shape, y_census_treinamento1.shape

#15% dados p teste,lin x col, [sem OHE: 14, com OHE: 116]
X_census_teste1.shape, y_census_teste1.shape

#Salvar as variáveis
import pickle
#crio arquivo .pkl 
with open('census1.pkl', mode = 'wb') as f:
  pickle.dump([X_census_treinamento1, y_census_treinamento1, X_census_teste1, y_census_teste1], f)

#--arvore de decisão RandomForest--#
#carregar a bib sklearn
from sklearn.ensemble import RandomForestClassifier

X_census_treinamento1.shape, y_census_treinamento1.shape

X_census_teste1.shape, y_census_teste1.shape

y_census_treinamento1

#gera randn forest nestimators [n de arvores] critrion [tipo de heuristica] fit[trinamento da arvore]
random_forest_census = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state = 0)
random_forest_census.fit(X_census_treinamento1, y_census_treinamento1)

previsoes = random_forest_census.predict(X_census_teste1)
previsoes

y_census_teste1

from sklearn.metrics import accuracy_score, classification_report
accuracy_score(y_census_teste1, previsoes)

from yellowbrick.classifier import ConfusionMatrix
cm = ConfusionMatrix(random_forest_census)
cm.fit(X_census_treinamento1, y_census_treinamento1)
cm.score(X_census_teste1, y_census_teste1)

print(classification_report(y_census_teste1, previsoes))

#desenha a arvore
from sklearn import tree
figura, eixos = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
tree.plot_tree(arvore_census, class_names=arvore_census.classes_, filled=True);

"""# ExtraTree

"""

#importar bib
from sklearn import metrics as m
from sklearn import ensemble as e

#cria as variaveis p treinamento e teste. 
#x atributos previsores, y p classes. test-size 15% p testar,85% p treinar 
X_census_treinamento2, X_census_teste2, y_census_treinamento2, y_census_teste2 = train_test_split(X_census, y_census, test_size = 20, random_state = 100)

#x atributos previsores, y p classes.
#exibe linhasxcol, #85% dados treinamento [sem oneHotEncode: 14 colunas, com OHE:116 col]
X_census_treinamento2.shape, y_census_treinamento2.shape

#15% dados p teste,lin x col, [sem OHE: 14, com OHE: 116]
X_census_teste2.shape, y_census_teste2.shape

#arvore de decisão normal #decisionTreeClassifier
#carregar a bib sklearn
from sklearn import tree as t

#cria 1 arvore somente com 1 nivel
#acuracia de 70%
one_tree = t.DecisionTreeClassifier( max_depth=1)
#training
model_one_tree = one_tree.fit(X_census_treinamento2, np.ravel(y_census_treinamento2))
#predict
pred_one_tree = model_one_tree.predict(X_census_teste2)
#performance
#acuracia de 70% p arvore c 1 folha só de aprendizado. aixo aprendizado
acc_one_tree = m.accuracy_score(y_census_teste2, pred_one_tree)
acc_one_tree

#multiplas arvores [Forest]
#treinar varias arvores com o mesmo tamamho [1 nivel] usando todos os dados e todos os recursos[features] p melhorar acuracia
#one tree
one_tree = t.DecisionTreeClassifier( max_depth=1)

#multiple tree 
#BaggingClassifier, n estimator= n de copias da arvore
multi_tree = e.BaggingClassifier (base_estimator = one_tree, n_estimators = 1000, bootstrap=False)
#training
#treinando mil arvores com 1 nivel. mesmaa curacia, pq ñ há variabilidade dos dados. ex: mil vezes respondendo idade: 18 anos. se combinar as mil respostas sempre
#será 18. ñ consigo explorar o dataset de outras formas p ter aprendizado. 
#aprende.
model_multi_tree = multi_tree.fit(X_census_treinamento2, np.ravel(y_census_treinamento2))
#predict
pred_multi_tree = model_multi_tree.predict(X_census_teste2)
#performance
#acuracia de 70% p arvore c 1 folha só de aprendizado. aixo aprendizado
acc_multi_tree = m.accuracy_score(y_census_teste2, pred_multi_tree)
acc_multi_tree

"""Estratégia 1: testar estratégia com subconjuts [datasets] com repetição de valores. amostras pode conter linhas repetidas
condições do experimento: 1.treinamento utilizando amostras com repetição. 2.Aleatoriamente escolher algumas features p ser testadas p determinar os nós 
da arvore, definido pela raiz quadrada de todos os classificadores. ex: 4 colunas seria raiz quadrada de 4 = 2. no nosso ex. raiz quadrada de 116 = 10.77 colunas
pq ocorra variabilidade e ela aprenda. no final pegar o conhec dessas 1k arvores e combinar no conhec coletivo , por meio de media.

"""

#Random Forest multiplas arvores
#treinar varias arvores com o mesmo tamamho [2 niveis], splinter=best usa um criterio p fazer a divisão dos dados. max-features=sqrt define a raiz quadrada. 
#add variabilidade no dataset. 
#acuracia: 75%

one_tree = t.DecisionTreeClassifier( max_depth=2, splitter='best', max_features='sqrt')

#multiple tree . bootstrap é uma amostragem com repetição 
#BaggingClassifier, n estimator= n de copias da arvore
multi_tree = e.BaggingClassifier (base_estimator = one_tree, n_estimators = 1000, bootstrap=True)
#training
model_multi_tree = multi_tree.fit(X_census_treinamento2, np.ravel(y_census_treinamento2))
#predict
pred_multi_tree = model_multi_tree.predict(X_census_teste2)
#performance
#acuracia de 70% p arvore c 1 folha só de aprendizado. aixo aprendizado
acc_multi_tree = m.accuracy_score(y_census_teste2, pred_multi_tree)
acc_multi_tree

"""Estrategia 2: treinar varias arvores com o mesmo tamamho [3 nivel], sem repetição.  
aleatoriamente algumas features serao escolhidas, e a divisão será aleatória, p determinar os nos da arvore. ñ usa criterio de gini/entropia p fazer a divisão
dos dados, essa divisão dos nós é feita aleatoriamente. vai add variabilidade nos classificador e ñ nos dados.
[-1k arv, 1 nó, acuracia 70%]
[-1k arv, 2 nós,ac: 75%]
[-1k arv, 3 nós, ac 80%]
"""

#Extra Trees [multiplas arvores sem repetição]
#splinter=aleatory usa um criterio aleatorio p fazer a divisão dos dados. max-features=sqrt define a raiz quadrada.
#acuracia: 80%
one_tree = t.DecisionTreeClassifier( max_depth=3, splitter='random')

#multiple tree . bootstrap false, é uma amostragem sem repetição 
#BaggingClassifier, n estimator= n de copias da arvore
multi_tree = e.BaggingClassifier (base_estimator = one_tree, n_estimators = 1000, bootstrap=False)
#training
model_multi_tree = multi_tree.fit(X_census_treinamento2, np.ravel(y_census_treinamento2))
#predict
pred_multi_tree = model_multi_tree.predict(X_census_teste2)
#performance
#acuracia de 70% p arvore c 1 folha só de aprendizado. aixo aprendizado
acc_multi_tree = m.accuracy_score(y_census_teste2, pred_multi_tree)
acc_multi_tree